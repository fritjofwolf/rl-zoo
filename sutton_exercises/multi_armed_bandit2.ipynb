{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-stationary Multi-Armed Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2.5 (programming) Design and conduct an experiment to demonstrate the difficulties that\n",
    "sample-average methods have for nonstationary problems. Use a modified version of the 10-armed\n",
    "testbed in which all the q ∗ (a) start out equal and then take independent random walks (say by adding\n",
    "a normally distributed increment with mean zero and standard deviation 0.01 to all the q ∗ (a) on each\n",
    "step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally\n",
    "computed, and another action-value method using a constant step-size parameter, α = 0.1. Use ε = 0.1\n",
    "and longer runs, say of 10,000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._means = np.zeros(10)\n",
    "        self._optimal_action = np.argmax(self._means)\n",
    "        \n",
    "    def pull_arm(self, arm):\n",
    "        return np.random.randn() + self._means[arm]\n",
    "    \n",
    "    def get_optimal_action(self):\n",
    "        return self._optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action(eps, action_values):\n",
    "    if np.random.rand() < eps:\n",
    "        return np.random.randint(10)\n",
    "    else:\n",
    "        return np.argmax(action_values)\n",
    "    \n",
    "def update_action_values(action_values, action_selected, next_action, next_reward):\n",
    "    action_values[next_action] += 1 / action_selected[next_action] * (next_reward-action_values[next_action])\n",
    "    action_selected[next_action] += 1\n",
    "    \n",
    "def update_statistics(rewards, optimal_action_chosen, next_reward, next_action, eps, bandit, step):\n",
    "    if next_action == optimal_action:\n",
    "        optimal_action_chosen[step, bandit, eps] = 1\n",
    "    rewards[step, bandit, eps] = next_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps = 5000\n",
    "rewards = np.zeros((n_timesteps,2000, 3))\n",
    "optimal_action_chosen = np.zeros((n_timesteps,2000, 3))\n",
    "eps_values = [0,0.01, 0.1]\n",
    "\n",
    "for idx, eps in enumerate(eps_values):\n",
    "    for bandit in range(2000):\n",
    "        mba = MultiArmedBandit()\n",
    "        optimal_action = mba.get_optimal_action()\n",
    "        action_values = np.zeros(10)\n",
    "        action_selected = np.ones(10)\n",
    "        for step in range(n_timesteps):\n",
    "            next_action = compute_action(eps, action_values)\n",
    "            next_reward = mba.pull_arm(next_action)\n",
    "            update_action_values(action_values, action_selected, next_action, next_reward)\n",
    "            update_statistics(rewards, optimal_action_chosen, next_reward, next_action, idx, bandit, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rewards = np.zeros((n_timesteps,3))\n",
    "average_rewards = np.mean(rewards, axis = 1)\n",
    "for i in range(3):\n",
    "    average_rewards[:,i] = [x / y for (x,y) in zip(np.cumsum(average_rewards[:,i]), np.arange(1,n_timesteps+1))]\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for i in range(3):\n",
    "    plt.plot(range(1,n_timesteps+1),average_rewards[:,i])\n",
    "plt.title('Average reward for different exploration rates with sample-average action values')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.legend(['Greedy', 'eps = 0.01', 'eps = 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_optimal_actions_chosen = np.cumsum(optimal_action_chosen, axis = 0)\n",
    "sum_optimal_actions_chosen = np.mean(sum_optimal_actions_chosen, axis = 1)\n",
    "for i in range(3):\n",
    "    sum_optimal_actions_chosen[:,i] = [x / y for (x,y) in \\\n",
    "                                       zip(sum_optimal_actions_chosen[:,i], np.arange(1,n_timesteps+1))]\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "for i in range(3):\n",
    "    plt.plot(range(1,n_timesteps+1),100*sum_optimal_actions_chosen[:,i])\n",
    "plt.title('Percentage of optimal action taken for different exploration rates with sample-average action values')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('% of Optimal Action')\n",
    "plt.legend(['Greedy', 'eps = 0.01', 'eps = 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
