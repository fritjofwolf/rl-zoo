{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "import tqdm\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \n",
    "    def __init__(self, env, gamma = 0.9, replay_buffer_size = 1000, learning_rate = 0.0003):\n",
    "        self._env = env\n",
    "        self._gamma = gamma\n",
    "        self._learning_rate = learning_rate\n",
    "        self._replay_buffer_size = replay_buffer_size\n",
    "        self._graph = {}\n",
    "        self._obs_dim = self._env.observation_space.shape[0]\n",
    "        self._n_acts = self._env.action_space.shape[0]\n",
    "        self._evaluation_scores = []\n",
    "        self._build_computational_graph_continuous_actions()\n",
    "\n",
    "    def train(self, n_iterations):\n",
    "        pass\n",
    "    \n",
    "    def _build_computational_graph_continuous_actions(self):\n",
    "        cg = self._graph\n",
    "        self._define_replay_buffer()\n",
    "        self._define_placeholder()\n",
    "        self._define_networks()\n",
    "        \n",
    "        action_means = cg['actor_network'](cg['obs_ph'])\n",
    "        tfd = tfp.distributions\n",
    "        dist = tfd.Normal(loc=action_means, scale=np.ones(self._n_acts))\n",
    "        cg['actions'] = dist.sample(1)[0]\n",
    "        \n",
    "        action_means_target = cg['actor_target'](cg['new_obs_ph'])\n",
    "        critic_input =  tf.concat([new_obs_ph, action_means_target], axis = 1)\n",
    "        y = rew_ph + self._gamma * cg['critic_target'](critic_input) * (1-terminal_ph)\n",
    "        old_action_values = cg['critic_network'](tf.concat([obs_ph, act_ph], axis = 1))\n",
    "        action_value_loss = tf.reduce_mean((tf.stop_gradient(y)-old_action_values)**2)\n",
    "        # todo: define policy loss\n",
    "        \n",
    "        self._define_optimizer()\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def _define_replay_buffer(self):\n",
    "        memory_width = 3 + 2*self._obs_dim\n",
    "        self._experience_replay_buffer = np.zeros((self._replay_buffer_size, memory_width)),\n",
    "        self._row_pointer = 0\n",
    "        \n",
    "    def _define_placeholder(self):\n",
    "        self._graph['obs_ph'] = tf.placeholder(shape=(None, self._obs_dim), dtype=tf.float32)\n",
    "        self._graph['act_ph'] = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "        self._graph['new_obs_ph'] = tf.placeholder(shape=(None, self._obs_dim), dtype=tf.float32)\n",
    "        self._graph['rew_ph'] = tf.placeholder(shape=(None,1), dtype=tf.float32)\n",
    "        self._graph['terminal_ph'] = tf.placeholder(shape=(None,1), dtype=tf.float32)\n",
    "\n",
    "    def _define_networks(self):\n",
    "        self._graph['critic_network'] = self._build_dense_model()\n",
    "        self._graph['critic_target'] = self._build_dense_model()\n",
    "        self._graph['actor_network'] = self._build_dense_model(self._n_acts, 'tanh')\n",
    "        self._graph['actor_target'] = self._build_dense_model(self._n_acts, 'tanh')\n",
    "        \n",
    "    def _build_dense_model(self, n_outputs = 1, activation = 'relu'):\n",
    "        mlp = tf.keras.models.Sequential()\n",
    "        mlp.add(tf.keras.layers.Dense(50, activation=activation))\n",
    "        mlp.add(tf.keras.layers.Dense(50, activation=activation))\n",
    "        mlp.add(tf.keras.layers.Dense(n_outputs, activation=None))\n",
    "        return mlp\n",
    "    \n",
    "    def _define_optimizer(self):\n",
    "        policy_optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        self._graph['train_policy'] = policy_optimizer.minimize(self._graph['policy_loss'])\n",
    "        action_value_optimizer = tf.train.AdamOptimizer(self._learning_rate)\n",
    "        self._graph['train_action_value'] = action_value_optimizer.minimize(self._graph['action_value_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WalkerBase::__init__ start\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HalfCheetahBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janus/repos/rl-zoo/venv/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "model = DDPG(env)\n",
    "model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
