{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_data_single_actor(sess, batch_size):\n",
    "#     # make some empty lists for logging.\n",
    "#     batch_obs = []          # for observations\n",
    "#     batch_acts = []         # for actions\n",
    "#     batch_rets = []         # for measuring episode returns\n",
    "#     batch_lens = []         # for measuring episode lengths\n",
    "#     batch_new_obs = []\n",
    "#     batch_rews = []\n",
    "#     batch_terminal = []\n",
    "    \n",
    "#     # reset episode-specific variables\n",
    "#     obs = env.reset()       # first obs comes from starting distribution\n",
    "#     done = False            # signal from environment that episode is over\n",
    "#     ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "#     # collect experience by acting in the environment with current policy\n",
    "#     while True:\n",
    "#         # save obs\n",
    "#         batch_obs.append(obs.copy())\n",
    "\n",
    "#         # act in the environment\n",
    "#         act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "#         obs, rew, done, info = env.step(act)\n",
    "#         batch_new_obs.append(obs.copy())\n",
    "#         # save action, reward\n",
    "#         batch_terminal.append(float(done))\n",
    "#         batch_acts.append(act)\n",
    "#         ep_rews.append(rew)\n",
    "\n",
    "#         if done:\n",
    "#             if len(ep_rews) == 200:\n",
    "#                 batch_terminal[-1] = 0.0\n",
    "#             # if episode is over, record info about episode\n",
    "#             ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "#             batch_rets.append(ep_ret)\n",
    "#             batch_rews.extend(ep_rews)\n",
    "#             batch_lens.append(ep_len)\n",
    "            \n",
    "#             # reset episode-specific variables\n",
    "#             obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "#             # end experience loop if we have enough of it\n",
    "#             if len(batch_obs) > batch_size:\n",
    "#                 break\n",
    "#     return batch_obs, batch_acts, batch_new_obs, batch_rews, batch_rets, batch_lens, batch_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = 2\n",
    "gamma = 0.9\n",
    "\n",
    "# placeholder\n",
    "obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "new_obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "rew_ph = tf.placeholder(shape=(None,1), dtype=tf.float32)\n",
    "terminal_ph = tf.placeholder(shape=(None,1), dtype=tf.float32)\n",
    "\n",
    "# make core of policy network\n",
    "mlp_policy = tf.keras.models.Sequential()\n",
    "mlp_policy.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp_policy.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp_policy.add(tf.keras.layers.Dense(n_acts))\n",
    "\n",
    "# make core of policy network\n",
    "mlp_policy_old = tf.keras.models.Sequential()\n",
    "mlp_policy_old.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp_policy_old.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp_policy_old.add(tf.keras.layers.Dense(n_acts))\n",
    "\n",
    "# make state-value function network\n",
    "mlp_state_value = tf.keras.models.Sequential()\n",
    "mlp_state_value.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "mlp_state_value.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "mlp_state_value.add(tf.keras.layers.Dense(1))\n",
    "state_value = mlp_state_value(obs_ph)\n",
    "new_state_value = mlp_state_value(new_obs_ph)\n",
    "td_target = rew_ph + gamma * new_state_value * (1-terminal_ph)\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "obs_logits = mlp_policy(obs_ph)\n",
    "old_obs_logits = mlp_policy_old(obs_ph)\n",
    "actions = tf.squeeze(tf.multinomial(logits=obs_logits,num_samples=1), axis=1)\n",
    "action_masks = tf.one_hot(act_ph, n_acts)\n",
    "selected_action_probs = tf.reduce_sum(action_masks * tf.nn.softmax(obs_logits), axis=1)\n",
    "old_selected_action_probs = tf.reduce_sum(action_masks * tf.nn.softmax(old_obs_logits), axis=1)\n",
    "\n",
    "r = selected_action_probs / tf.stop_gradient(old_selected_action_probs)\n",
    "advantages = tf.squeeze(td_target - state_value, axis=1)\n",
    "factor = 1 + 0.2 * tf.math.sign(advantages)\n",
    "x = tf.math.minimum(advantages*r, advantages*factor)\n",
    "policy_loss = -tf.reduce_mean(x)\n",
    "\n",
    "# state value loss function\n",
    "y  = (tf.stop_gradient(td_target) - state_value)**2\n",
    "state_value_loss = tf.reduce_mean(y)\n",
    "\n",
    "\n",
    "# debug\n",
    "average_state_value = tf.reduce_mean(state_value)\n",
    "max_r = tf.reduce_max(r)\n",
    "max_advantages = tf.reduce_max(advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_old_network(old_policy_network, policy_network):\n",
    "    sess.run([v_t.assign(v) for v_t, v in zip(old_policy_network.trainable_weights, policy_network.trainable_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollector():\n",
    "    \n",
    "    def __init__(self, sess, env_name, actions, obs_ph, n_actors, n_samples):\n",
    "        self._sess = sess\n",
    "        self._envs = [gym.make(env_name) for _ in range(n_actors)]\n",
    "        self._states = [env.reset() for env in self._envs]\n",
    "        self._actor_rews = [[] for _ in range(n_actors)]\n",
    "        self._n_samples = n_samples\n",
    "        self._actions = actions\n",
    "        self._obs_ph = obs_ph\n",
    "        self._returns = []\n",
    "        self._lens = []\n",
    "        \n",
    "    def collect_data(self):\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_new_obs = []\n",
    "        batch_rews = []\n",
    "        batch_terminal = []\n",
    "        for i in range(len(self._envs)):\n",
    "            tmp_data = self._collect_data_single_actor(i)\n",
    "            batch_obs.extend(tmp_data[0])\n",
    "            batch_acts.extend(tmp_data[1])            \n",
    "            batch_new_obs.extend(tmp_data[2])            \n",
    "            batch_rews.extend(tmp_data[3])\n",
    "            batch_terminal.extend(tmp_data[4])\n",
    "        return batch_obs, batch_acts, batch_new_obs, batch_rews, batch_terminal  \n",
    "    \n",
    "    def print_return_statistics(self):\n",
    "        print('Statistics of the last 100 episodes:')\n",
    "        ret_mean = np.mean(self._returns[-100:])\n",
    "        ret_std = np.std(self._returns[-100:])\n",
    "        ret_min = np.min(self._returns[-100:])\n",
    "        ret_max = np.max(self._returns[-100:])\n",
    "        print(ret_mean, ret_std, ret_min, ret_max)\n",
    "    \n",
    "    def _collect_data_single_actor(self, env_id):\n",
    "        batch_obs = []\n",
    "        batch_acts = []\n",
    "        batch_new_obs = []\n",
    "        batch_rews = []\n",
    "        batch_terminal = []\n",
    "        \n",
    "        env = self._envs[env_id]\n",
    "        obs = self._states[env_id]\n",
    "        done = False\n",
    "        \n",
    "        for _ in range(self._n_samples):\n",
    "            batch_obs.append(obs.copy())\n",
    "            act = self._sess.run(self._actions, {self._obs_ph: obs.reshape(1,-1)})[0]\n",
    "            obs, rew, done, info = env.step(act)\n",
    "            batch_new_obs.append(obs.copy())\n",
    "            batch_terminal.append(float(done))\n",
    "            batch_acts.append(act)\n",
    "            batch_rews.append(rew)\n",
    "            self._actor_rews[env_id].append(rew)\n",
    "            if done:\n",
    "                ep_len = len(self._actor_rews[env_id])\n",
    "                ep_ret = sum(self._actor_rews[env_id])\n",
    "                self._returns.append(ep_ret)\n",
    "                self._lens.append(ep_len)\n",
    "                self._actor_rews[env_id] = []\n",
    "                \n",
    "#                 if ep_len == 200:\n",
    "#                     batch_terminal[-1] = 0.0\n",
    "                obs, done= env.reset(), False\n",
    "        self._states[env_id] = obs\n",
    "        return batch_obs, batch_acts, batch_new_obs, batch_rews, batch_terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "policy_optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "state_value_optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "train_policy = policy_optimizer.minimize(policy_loss)\n",
    "train_state_value = state_value_optimizer.minimize(state_value_loss)\n",
    "\n",
    "n_epochs = 200\n",
    "K = 5\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "data_collector = DataCollector(sess, 'CartPole-v0', actions, obs_ph, 20, 50)\n",
    "for i in range(n_epochs):\n",
    "    update_old_network(mlp_policy_old, mlp_policy)\n",
    "    obs, acts, new_obs, rews, terminal = data_collector.collect_data()\n",
    "    data_collector.print_return_statistics()\n",
    "    for j in range(K):\n",
    "        print(sess.run([max_r,max_advantages], feed_dict ={\n",
    "                obs_ph: np.array(obs).reshape(-1, obs_dim),\n",
    "                act_ph: np.array(acts),\n",
    "                new_obs_ph: np.array(new_obs).reshape(-1, obs_dim),\n",
    "                rew_ph: np.array(rews).reshape(-1, 1),\n",
    "                terminal_ph: np.array(terminal).reshape(-1, 1)\n",
    "        }))\n",
    "        sess.run([train_policy],feed_dict={\n",
    "                                    obs_ph: np.array(obs).reshape(-1, obs_dim),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    new_obs_ph: np.array(new_obs).reshape(-1, obs_dim),\n",
    "                                    rew_ph: np.array(rews).reshape(-1, 1),\n",
    "                                    terminal_ph: np.array(terminal).reshape(-1, 1)\n",
    "                                 })\n",
    "    for j in range(30):\n",
    "        sess.run([train_state_value],feed_dict={\n",
    "                                    obs_ph: np.array(obs).reshape(-1, obs_dim),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    new_obs_ph: np.array(new_obs).reshape(-1, obs_dim),\n",
    "                                    rew_ph: np.array(rews).reshape(-1, 1),\n",
    "                                    terminal_ph: np.array(terminal).reshape(-1, 1)\n",
    "                                 })\n",
    "    print('State value loss is:')\n",
    "    print(sess.run(state_value_loss, feed_dict ={\n",
    "                obs_ph: np.array(obs).reshape(-1, obs_dim),\n",
    "                act_ph: np.array(acts),\n",
    "                new_obs_ph: np.array(new_obs).reshape(-1, obs_dim),\n",
    "                rew_ph: np.array(rews).reshape(-1, 1),\n",
    "                terminal_ph: np.array(terminal).reshape(-1, 1)\n",
    "        }))\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "env._max_episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
