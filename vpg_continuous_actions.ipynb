{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXOLwVpnriyu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import pybullet_envs\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from src.data_collection import collect_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open problems:\n",
    "    - use multiprocessing to compute several trajectories at the same time\n",
    "    - find number actions/ observations automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKRYbCNYre-O"
   },
   "outputs": [],
   "source": [
    "def collect_data(sess, batch_size, gamma = 0.99, render=False):\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "    # reset episode-specific variables\n",
    "    obs = env.reset()       # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "        \n",
    "        obs[2] /= 8\n",
    "        batch_obs.append(obs.copy())\n",
    "\n",
    "        # act in the environment\n",
    "        act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "        # save action, reward\n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            env.close()\n",
    "            render = False\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            # the weight for each logprob(a_t|s_t) is reward-to-go from t\n",
    "#             batch_weights += list(np.cumsum(ep_rews[::-1])[::-1])\n",
    "            obs[2] /= 8\n",
    "            bootstrap_value = sess.run(state_values, {obs_ph:obs.reshape(1,-1)})[0][0]\n",
    "            batch_weights += compute_rewards_to_go(ep_rews, gamma, bootstrap_value)\n",
    "            \n",
    "            # reset episode-specific variables\n",
    "            obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "    return batch_obs, batch_acts, batch_weights, batch_rets, batch_lens\n",
    "\n",
    "def compute_rewards_to_go(rewards, gamma, bootstrap_value):\n",
    "    rewards_to_go = [rewards[-1] + gamma*bootstrap_value]\n",
    "    for rew in rewards[:-1][::-1]:\n",
    "        tmp = rewards_to_go[-1]\n",
    "        rewards_to_go.append(rew + gamma * tmp)\n",
    "    return rewards_to_go[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_compute_rewards_to_go():\n",
    "    rewards = [128,1024,8]\n",
    "    gamma = 0.5\n",
    "    bootstrap_value = 32\n",
    "    expected_output = [128+512+2+4, 1024+4+8, 8+16]\n",
    "    assert(compute_rewards_to_go(rewards, gamma, bootstrap_value) == expected_output)\n",
    "\n",
    "test_compute_rewards_to_go()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SwEgl3AsRS1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dir=/home/research/repos/rl-zoo/venv/lib/python3.6/site-packages/pybullet_envs/bullet\n",
      "urdf_root=/home/research/repos/rl-zoo/venv/lib/python3.6/site-packages/pybullet_data\n",
      "options= \n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('Pendulum-v0')\n",
    "# env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "import pybullet_envs.bullet.minitaur_gym_env as e\n",
    "env = e.MinitaurBulletEnv(render=True)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.shape[0]\n",
    "\n",
    "# placeholder\n",
    "obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "act_ph = tf.placeholder(shape=(None,n_acts), dtype=tf.float32)\n",
    "weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "\n",
    "# network for gaussian means\n",
    "mlp = tf.keras.models.Sequential()\n",
    "mlp.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp.add(tf.keras.layers.Dense(50, activation='tanh'))\n",
    "mlp.add(tf.keras.layers.Dense(n_acts))\n",
    "means = mlp(obs_ph)\n",
    "\n",
    "# value function network\n",
    "state_value_mlp = tf.keras.models.Sequential()\n",
    "state_value_mlp.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "state_value_mlp.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "state_value_mlp.add(tf.keras.layers.Dense(1))\n",
    "state_values = state_value_mlp(obs_ph)\n",
    "\n",
    "# variances\n",
    "log_std = tf.Variable(-0.5)\n",
    "std = tf.math.exp(log_std)\n",
    "# compute actions\n",
    "actions = tf.random.normal((1,1), mean=means, stddev=std)\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "first_summand = tf.reduce_sum(((act_ph - means) / std)**2 + 2*log_std)\n",
    "log_probs = -0.5*(first_summand + n_acts * tf.math.log(2*np.pi))\n",
    "loss = -tf.reduce_mean((weights_ph - state_values) * log_probs)\n",
    "\n",
    "state_value_loss = tf.reduce_mean((weights_ph - state_values)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3859
    },
    "colab_type": "code",
    "id": "d7BvcPvltv-i",
    "outputId": "6e8e6588-c84b-4c7e-f54c-067f9023674f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration, min, max, mean, std\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b81102f687ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iteration, min, max, mean, std\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_rets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4000\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_rets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     sess.run([train],feed_dict={\n",
      "\u001b[0;32m<ipython-input-2-cf0d1d60c5dc>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(sess, batch_size, gamma, render)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# act in the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# save action, reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/rl-zoo/venv/lib/python3.6/site-packages/pybullet_envs/bullet/minitaur_gym_env.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_to_sleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mbase_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminitaur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetBasePosition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mcamInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pybullet_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDebugVisualizerCamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0mcurTargetPos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m       \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcamInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "train = optimizer.minimize(loss)\n",
    "state_value_optimizer = tf.train.AdamOptimizer(0.001)\n",
    "state_value_train = state_value_optimizer.minimize(state_value_loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "episode_returns = []\n",
    "\n",
    "print('Iteration, min, max, mean, std\\n')\n",
    "for i in range(200):\n",
    "    observations, acts, weights, batch_rets, batch_len = collect_data(sess, 4000 ,render=True)\n",
    "    print(i, np.mean(batch_rets), np.std(batch_rets), np.min(batch_rets), np.max(batch_rets))\n",
    "    sess.run([train],feed_dict={\n",
    "                                    obs_ph: np.array(observations),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    weights_ph: np.array(weights)\n",
    "                                 })\n",
    "    for _ in range(50):\n",
    "        sess.run([state_value_train],feed_dict={\n",
    "                                    obs_ph: np.array(observations),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    weights_ph: np.array(weights)\n",
    "                                 })\n",
    "    print(sess.run(state_values ,feed_dict={\n",
    "                                    obs_ph: np.array(observations),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    weights_ph: np.array(weights)\n",
    "                                 }))\n",
    "#     print(sess.run(means,feed_dict={\n",
    "#                                     obs_ph: np.array([[0,0,0]]),\n",
    "#                                  }))\n",
    "    print(sess.run(state_value_loss, feed_dict={\n",
    "                                    obs_ph: np.array(observations),\n",
    "                                    act_ph: np.array(acts),\n",
    "                                    weights_ph: np.array(weights)\n",
    "        }))\n",
    "    print()\n",
    "#     print('State value function loss:', svloss)\n",
    "#     print(tmp2)\n",
    "    #print(sess.run([means,std],feed_dict={\n",
    "#                                     obs_ph: np.array(tmp1),\n",
    "#                                     act_ph: np.array(tmp2),\n",
    "#                                     weights_ph: np.array(tmp3)\n",
    "#                         }))\n",
    "print('Evaluation')\n",
    "tmp1, tmp2, tmp3, batch_rets, batch_len = collect_data(sess, 5, render=True)\n",
    "print(np.mean(batch_len), np.min(batch_len), np.max(batch_len))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ta2MN0ZJRufF"
   },
   "outputs": [],
   "source": [
    "t = 100\n",
    "episode_mean_returns = [np.mean(episode_returns[i-t:i]) for i in range(t, len(episode_returns))]\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(range(t,len(episode_mean_returns)+t), episode_mean_returns, color=np.random.rand(3))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Mean Episode Return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_dim = 4\n",
    "# n_acts = 2\n",
    "# env = gym.make('CartPole-v0')\n",
    "\n",
    "# # placeholder\n",
    "# obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "# act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "# weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "\n",
    "# # make core of policy network\n",
    "# mlp = tf.keras.models.Sequential()\n",
    "# mlp.add(tf.keras.layers.Dense(30, activation='tanh'))\n",
    "# mlp.add(tf.keras.layers.Dense(n_acts))\n",
    "# logits = mlp(obs_ph)\n",
    "\n",
    "# # # make state-value function network\n",
    "# # mlp_val = tf.keras.models.Sequential()\n",
    "# # mlp_val.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "# # mlp_val.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "# # mlp_val.add(tf.keras.layers.Dense(1))\n",
    "# # state_values = mlp_val(obs_ph)\n",
    "\n",
    "# # make action selection op (outputs int actions, sampled from policy)\n",
    "# actions = tf.squeeze(tf.multinomial(logits=logits,num_samples=1), axis=1)\n",
    "\n",
    "# # make loss function whose gradient, for the right data, is policy gradient\n",
    "# action_masks = tf.one_hot(act_ph, n_acts)\n",
    "# log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
    "# loss = -tf.reduce_mean(weights_ph * log_probs)\n",
    "\n",
    "\n",
    "# state_value_loss = tf.reduce_mean(((weights_ph - state_values) - state_values)**2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of simple_policy_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
