{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "# import numpy as np\n",
    "# from src.data_collection import collect_data\n",
    "import logging\n",
    "import tqdm\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPG():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        if type(env.action_space) == gym.spaces.box.Box:\n",
    "            self._build_computational_graph_continuous_actions()\n",
    "        else:\n",
    "            self._build_computational_graph_categorical_actions()   \n",
    "\n",
    "\n",
    "    def train(self, policy_learning_rate=0.0003, value_function_learning_rate = 0.001, n_value_function_updates = 10,\\\n",
    "              n_epochs = 10):\n",
    "        logging.basicConfig(filename='training.log',level=logging.DEBUG)\n",
    "        logging.debug('Current Epoch, mean return, std return, min return, max return')\n",
    "\n",
    "        episode_returns = []\n",
    "        epoch_state_value_loss = []\n",
    "        epoch_entropy = []\n",
    "\n",
    "        \n",
    "        for i in tqdm.tqdm(range(n_epochs)):\n",
    "            batch_obs, batch_acts, batch_rews, batch_rets, batch_len = collect_data(self._env, self._graph, 4000)\n",
    "            episode_returns.extend(batch_rets)\n",
    "            logging.debug('%i, %f, %f, %f, %f',i, np.mean(batch_rets), np.std(batch_rets), np.min(batch_rets), np.max(batch_rets))\n",
    "\n",
    "            sess.run(self._graph['train_policy'],feed_dict={\n",
    "                                            self._graph['obs_ph']: np.array(batch_obs),\n",
    "                                            self._graph['act_ph']: np.array(batch_acts),\n",
    "                                            self._graph['rew_ph']: np.array(batch_rews)\n",
    "                                        })\n",
    "            for _ in range(n_value_function_updates):\n",
    "                sess.run(self._graph['train_state_value'],feed_dict={\n",
    "                                        self._graph['obs_ph']: np.array(batch_obs),\n",
    "                                        self._graph['act_ph']: np.array(batch_acts),\n",
    "                                        self._graph['rew_ph']: np.array(batch_rews)\n",
    "                                    })\n",
    "            v, e = sess.run([self._graph['state_value_loss'], self._graph['entropy']], feed_dict={\n",
    "                                            self._graph['obs_ph']: np.array(batch_obs),\n",
    "                                            self._graph['act_ph']: np.array(batch_acts),\n",
    "                                            self._graph['rew_ph']: np.array(batch_rews)\n",
    "                                        })\n",
    "            epoch_state_value_loss.append(v)\n",
    "            epoch_entropy.append(e)\n",
    "        return episode_returns, epoch_state_value_loss, epoch_entropy\n",
    "\n",
    "\n",
    "    def _build_computational_graph_categorical_actions(self):\n",
    "        env = self._env\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        n_acts = env.action_space.n\n",
    "\n",
    "        # placeholder\n",
    "        obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "        act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "        weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)\n",
    "\n",
    "        # make core of policy network\n",
    "        mlp = tf.keras.models.Sequential()\n",
    "        mlp.add(tf.keras.layers.Dense(30, activation='tanh'))\n",
    "        mlp.add(tf.keras.layers.Dense(30, activation='tanh'))\n",
    "        mlp.add(tf.keras.layers.Dense(n_acts))\n",
    "        logits = mlp(obs_ph)\n",
    "\n",
    "        # value function network\n",
    "        state_value_mlp = tf.keras.models.Sequential()\n",
    "        state_value_mlp.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "        state_value_mlp.add(tf.keras.layers.Dense(50, activation='relu'))\n",
    "        state_value_mlp.add(tf.keras.layers.Dense(1))\n",
    "        state_values = state_value_mlp(obs_ph)\n",
    "\n",
    "        # make action selection op (outputs int actions, sampled from policy)\n",
    "        actions = tf.squeeze(tf.multinomial(logits=logits,num_samples=1), axis=1)\n",
    "        action_probs = tf.nn.softmax(logits)\n",
    "        entropy = -tf.reduce_mean(tf.reduce_sum(action_probs * tf.math.log(action_probs), axis=1))\n",
    "\n",
    "        # make loss function whose gradient, for the right data, is policy gradient\n",
    "        action_masks = tf.one_hot(act_ph, n_acts)\n",
    "        log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)\n",
    "        policy_loss = -tf.reduce_mean((weights_ph - state_values) * log_probs)\n",
    "\n",
    "        state_value_loss = tf.reduce_mean((weights_ph - state_values)**2)\n",
    "\n",
    "        optimizer_policy = tf.train.AdamOptimizer(policy_learning_rate)\n",
    "        train_policy = optimizer_policy.minimize(policy_loss)\n",
    "        optimizer_state_value = tf.train.AdamOptimizer(value_function_learning_rate)\n",
    "        train_state_value = optimizer_state_value.minimize(state_value_loss)\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        graph = {\n",
    "                    'obs_ph': obs_ph,\n",
    "                    'act_ph': act_ph,\n",
    "                    'rew_ph': rew_ph,\n",
    "                    'actions': actions,\n",
    "                    'state_values': state_values,\n",
    "                    'entropy': entropy,\n",
    "                    'policy_loss': policy_loss,\n",
    "                    'state_value_loss': state_value_loss,\n",
    "                    'train_policy': train_policy,\n",
    "                    'train_state_value': train_state_value,\n",
    "                    'sess': sess\n",
    "                }\n",
    "        self._graph = graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "vpg = VPG(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a23b4892596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ec29824579ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, policy_learning_rate, value_function_learning_rate, n_value_function_updates, n_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_value_function_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 sess.run([train_state_value],feed_dict={\n\u001b[0;32m---> 42\u001b[0;31m                                         \u001b[0mobs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                                         \u001b[0mact_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                         \u001b[0mweights_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def collect_data(env, computational_graph, batch_size, gamma = 0.99):\n",
    "    pass\n",
    "\n",
    "\n",
    "def collect_data_actor(env, computational_graph, batch_size, gamma = 0.99):\n",
    "    import numpy as np\n",
    "    # make some empty lists for logging.\n",
    "    batch_obs = []          # for observations\n",
    "    batch_acts = []         # for actions\n",
    "    batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "    batch_rets = []         # for measuring episode returns\n",
    "    batch_lens = []         # for measuring episode lengths\n",
    "    # reset episode-specific variables\n",
    "    obs = env.reset()       # first obs comes from starting distribution\n",
    "    done = False            # signal from environment that episode is over\n",
    "    ep_rews = []            # list for rewards accrued throughout ep\n",
    "    # collect experience by acting in the environment with current policy\n",
    "    while True:\n",
    "        batch_obs.append(obs.copy())\n",
    "\n",
    "        # act in the environment\n",
    "        act = sess.run(actions, {obs_ph: obs.reshape(1,-1)})[0]\n",
    "        obs, rew, done, _ = env.step(act)\n",
    "        \n",
    "        # save action, reward\n",
    "        batch_acts.append(act)\n",
    "        ep_rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "            if render:\n",
    "                env.close()\n",
    "                render = False\n",
    "            # if episode is over, record info about episode\n",
    "            ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "            batch_rets.append(ep_ret)\n",
    "            batch_lens.append(ep_len)\n",
    "            \n",
    "            if ep_len == env._max_episode_steps:\n",
    "                bootstrap_value = sess.run(state_values, {obs_ph:obs.reshape(1,-1)})[0][0]\n",
    "            else:\n",
    "                bootstrap_value = 0\n",
    "            batch_weights += compute_rewards_to_go(ep_rews, gamma, bootstrap_value)\n",
    "            \n",
    "            # reset episode-specific variables\n",
    "            obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "            # end experience loop if we have enough of it\n",
    "            if len(batch_obs) > batch_size:\n",
    "                break\n",
    "    return batch_obs, batch_acts, batch_weights, batch_rets, batch_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
