{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_policy(n_acts = 2):\n",
    "    mlp_action_val = tf.keras.models.Sequential()\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(30, activation=activation))\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(30, activation=activation))\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(n_acts, activation='softmax'))\n",
    "    return mlp_action_val\n",
    "\n",
    "def build_model_policy():\n",
    "    mlp_action_val = tf.keras.models.Sequential()\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "    mlp_action_val.add(tf.keras.layers.Dense(1, activation=None))\n",
    "    return mlp_action_val\n",
    "\n",
    "def compute_rewards_to_go(rewards, gamma, bootstrap_value):\n",
    "    rewards_to_go = [rewards[-1] + gamma*bootstrap_value]\n",
    "    for rew in rewards[:-1][::-1]:\n",
    "        tmp = rewards_to_go[-1]\n",
    "        rewards_to_go.append(rew + gamma * tmp)\n",
    "    return rewards_to_go[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorLearner():\n",
    "    \n",
    "    central_counter = 0\n",
    "    mlp_policy = build_model(activation= 'tanh')\n",
    "    mlp_state_value = build_model(activation = 'relu')\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._I_central_update = 10\n",
    "        self._env = gym.make('CartPole-v0')\n",
    "        self._state = self._env.reset()\n",
    "        self._counter = 0\n",
    "        self._reset()\n",
    "        self._sess = tf.Session()\n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def _build_computational_graph(self):\n",
    "        obs_dim = self._env.observation_space.shape[0]\n",
    "        n_acts = self._env.action_space.n\n",
    "        gamma = 0.9\n",
    "\n",
    "        obs_ph = tf.placeholder(shape=(None,obs_dim), dtype=tf.float32)\n",
    "        act_ph = tf.placeholder(shape=(None), dtype=tf.int32)\n",
    "        rew_ph = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "        terminal_ph = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "\n",
    "        # make core of state-action-value function network\n",
    "        mlp_target_policy = build_model_policy()\n",
    "        mlp_target_state_value = build_model_state_value()\n",
    "\n",
    "        state_values = mlp_target_state_value(obs_ph)\n",
    "        action_probs = mlp_target_policy(obs_ph)\n",
    "        greedy_action = tf.math.argmax(action_probs, axis=1)\n",
    "\n",
    "        # define loss functions\n",
    "        action_masks = tf.one_hot(act_ph, n_acts)\n",
    "        selected_action_probs = tf.reduce_sum(action_masks * action_probs, axis=1)\n",
    "        policy_loss = tf.reduce_sum(tf.math.log(selected_action_probs) * (rew_ph - tf.no_grad(state_values)), axis=1)\n",
    "        \n",
    "        state_value_loss = tf.losses.mean_squared_error(rew_ph, state_values)\n",
    "\n",
    "        # define optimizer\n",
    "        optimizer_action_value = tf.train.AdamOptimizer(0.0003)\n",
    "        train_policy = optimizer_action_value.minimize(policy_loss)\n",
    "        optimizer_action_value = tf.train.AdamOptimizer(0.001)\n",
    "        train_state_value = optimizer_action_value.minimize(state_value_loss)\n",
    "        \n",
    "        self._graph = [train_policy, train_state_value, obs_ph, act_ph, rew_ph, terminal_ph]\n",
    "        \n",
    "    def learning_step(self):\n",
    "        for i in range(self._I_central_update):\n",
    "            act, obs, rew, done = self._take_env_step()\n",
    "            self._save_infos(self._state, act, obs, rew, done)\n",
    "            self._update(obs, done)\n",
    "\n",
    "    def _reset(self):\n",
    "        self._obs = []\n",
    "        self._acts = []\n",
    "        self._rews = []\n",
    "    \n",
    "        \n",
    "    def _save_infos(self, old_state, action, new_state, reward, done):\n",
    "        if not done:\n",
    "            bootstrap_value = self._sess.run(state_values, feed_dict={obs_ph:new_state.reshape(1,-1)})\n",
    "        else:\n",
    "            bootstrap_value = 0\n",
    "        self._obs.append(old_state)\n",
    "        self._acts.append(action)\n",
    "        self._rews.append(reward + gamma * bootstrap_value)\n",
    "        \n",
    "    def _update(self, obs, done):\n",
    "        self._counter += 1\n",
    "        ActorLearner.central_counter += 1\n",
    "        if ActorLearner.central_counter % self._I_target_update == 0:\n",
    "            self._update_target_network()\n",
    "        if self._counter % self._I_central_update == 0 or done:\n",
    "            self._update_central_network()\n",
    "            self._reset()\n",
    "        if done:\n",
    "            done = False\n",
    "            self._state = self._env.reset()\n",
    "        else:\n",
    "            self._state = obs\n",
    "    \n",
    "    def _update_central_network(self):\n",
    "        _, loss = sess.run([train_action_value, action_value_loss], feed_dict={\n",
    "                    obs_ph: np.array(self._obs).reshape(-1, obs_dim),\n",
    "                    act_ph: np.array(self._acts),\n",
    "                    rew_ph: np.array(self._rews),\n",
    "                    terminal_ph: np.array(self._terminal_flags)\n",
    "                 })\n",
    "        #print(loss)\n",
    "    \n",
    "    def _update_target_network(self):\n",
    "        sess.run([v_t.assign(v) for v_t, v in zip(mlp_target.trainable_weights, mlp_action_val.trainable_weights)])\n",
    "    \n",
    "    def _take_env_step(self):\n",
    "        if np.random.rand() < 0.1:\n",
    "            act = np.random.randint(n_acts)\n",
    "        else:\n",
    "            act = sess.run(greedy_action, {obs_ph: self._state.reshape(1,-1)})[0]\n",
    "        obs, rew, done, info = self._env.step(act)\n",
    "        return act, obs, rew, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c27753b0a751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mweight_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_action_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/repos/rl-zoo/venv/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m   \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_ref_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    220\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "# computational graph\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "n_acts = env.action_space.n\n",
    "gamma = 0.9\n",
    "\n",
    "obs_ph = tf.placeholder(shape=(None,obs_dim), dtype=tf.float32)\n",
    "act_ph = tf.placeholder(shape=(None), dtype=tf.int32)\n",
    "rew_ph = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "new_obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)\n",
    "terminal_ph = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "\n",
    "# make core of state-action-value function network\n",
    "mlp_target_policy = build_model()\n",
    "mlp_target_state_value = build_model()\n",
    "\n",
    "# define state action values\n",
    "old_state_action_values = mlp_action_val(obs_ph)\n",
    "new_state_action_values = mlp_target(new_obs_ph)\n",
    "\n",
    "# select action\n",
    "greedy_action = tf.math.argmax(old_state_action_values, axis=1)\n",
    "\n",
    "# define loss function\n",
    "y = rew_ph + gamma * tf.reduce_max(new_state_action_values, axis=1)*(1-terminal_ph)\n",
    "y_no_grad = tf.stop_gradient(y) \n",
    "action_masks = tf.one_hot(act_ph, n_acts)\n",
    "old_selected_action_values = tf.reduce_sum(action_masks * old_state_action_values, axis=1)\n",
    "action_value_loss = tf.losses.mean_squared_error(y_no_grad, old_selected_action_values)\n",
    "\n",
    "# define optimizer\n",
    "optimizer_action_value = tf.train.AdamOptimizer(0.001)\n",
    "train_action_value = optimizer_action_value.minimize(action_value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluate(sess, env, iterations):\n",
    "    sum_return = 0\n",
    "    for i in range(iterations):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "#             if i == 0:\n",
    "#                 time.sleep(0.03)\n",
    "#                 env.render()\n",
    "            action = select_action(sess, state, 0)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            state = new_state\n",
    "            sum_return += reward\n",
    "        env.close()\n",
    "    return sum_return / iterations\n",
    "\n",
    "\n",
    "def select_action(sess, state, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        action = np.random.randint(n_acts)\n",
    "    else:\n",
    "        action = sess.run(greedy_action, {obs_ph: state.reshape(1,-1)})[0]\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-54.635651125137606\n",
      "-371.64285139015266\n",
      "-413.3254531292724\n",
      "-86.98586873524516\n",
      "-156.4931166974486\n",
      "-237.90129499636484\n"
     ]
    }
   ],
   "source": [
    "iterations = 100000\n",
    "n_learners = 1\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "learners = [ActorLearner() for i in range(n_learners)]\n",
    "for i in range(iterations):\n",
    "    learners[i%n_learners].learning_step()\n",
    "    if i % 1000 == 0:\n",
    "        print(evaluate(sess, env, 3))\n",
    "#         print(sess.run(old_state_action_values, feed_dict={\n",
    "#                         obs_ph: np.zeros(4).reshape(-1,obs_dim)\n",
    "#         }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
